{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5c1648-6268-4829-bbaf-53ef779bd1a0",
   "metadata": {},
   "source": [
    "# Tutorial 6: More Informative Rendering\n",
    "\n",
    "In this tutorial, we will discuss how to make rendering more informative. We will also demonstrate that rendering is agnostic to the selected physics-engine.\n",
    "\n",
    "The following will be covered:\n",
    "- Create an overlay node that augments a raw image sensors\n",
    "- Connect the overlay node and use it for rendering\n",
    "- Demonstrate that rendering is agnostic to the selected physics-engine\n",
    "\n",
    "In the remainder of this tutorial, we will go more into detail on this concept.\n",
    "\n",
    "Furthermore, at the end of this notebook you will find an exercise.\n",
    "For the exercise you will have to add/modify a couple of lines of code, which are marked by\n",
    "\n",
    "```python\n",
    "\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "## Pendulum Swing-up\n",
    "\n",
    "We will assume that we already have the object definition of the underactuated pendulum that we used in the [first](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb) tutorial with its dynamics simulated with the [OdeEngine](https://github.com/eager-dev/eagerx_ode). We will also assume that the *engine-specific* implementation we created in the [previous tutorial](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/5_engine_implementation.ipynb) for the [GymEngine](https://github.com/eager-dev/eagerx/blob/master/eagerx/engines/openai_gym/engine.py) is available.\n",
    "\n",
    "Our goal is to make the rendered images more informative. We will lay the actions, selected by the agent, over the raw images produced by the image sensor of the pendulum. We will then visualise the augmented images instead of the raw images from the image sensor.\n",
    "\n",
    "## Activate GPU (Colab only)\n",
    "\n",
    "When in Colab, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "In order to be able to run the code, we need to install the *eagerx_tutorials* package and ROS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8b6d03-40d4-4b45-92b2-61ffa09a3249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab.\n",
      "Execute ROS commands as \"!...\".\n",
      "ROS noetic available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import eagerx_tutorials\n",
    "except ImportError:\n",
    "    !{\"echo 'Installing eagerx-tutorials with pip.' && pip install eagerx-tutorials  >> /tmp/eagerx_install.txt 2>&1\"}\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !{\"curl 'https://raw.githubusercontent.com/eager-dev/eagerx_tutorials/master/scripts/setup_colab.sh' > ~/setup_colab.sh\"}\n",
    "  !{\"bash ~/setup_colab.sh\"}\n",
    "\n",
    "# Setup interactive notebook\n",
    "# Required in interactive notebooks only.\n",
    "from eagerx_tutorials import helper\n",
    "helper.setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d7676-c74e-4760-932e-4a347d7cfc5c",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "We start by importing the required packages and initializing EAGERx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da87b514-cde8-4ff3-968b-718fe1b5c38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... logging to /home/jelle/.ros/log/fde154f4-d6a3-11ec-8700-5feb7550a31d/roslaunch-jelle-Alienware-m15-R4-132754.log\n",
      "\u001b[1mstarted roslaunch server http://145.94.219.234:34113/\u001b[0m\n",
      "ros_comm version 1.15.14\n",
      "\n",
      "\n",
      "SUMMARY\n",
      "========\n",
      "\n",
      "PARAMETERS\n",
      " * /rosdistro: noetic\n",
      " * /rosversion: 1.15.14\n",
      "\n",
      "NODES\n",
      "\n",
      "[INFO] [1652877820.255265]: Roscore cannot run as another roscore/master is already running. Continuing without re-initializing the roscore.\n"
     ]
    }
   ],
   "source": [
    "import eagerx\n",
    "import eagerx_tutorials.pendulum  # Registers Pendulum\n",
    "import eagerx_tutorials.pendulum.gym_implementation  # Registers engine-specific implementation for the GymEngine  \n",
    "\n",
    "# Initialize eagerx (starts roscore if not already started.)\n",
    "eagerx.initialize(\"eagerx_core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff0b7a-16a8-4628-9fe0-0f16e3a3804a",
   "metadata": {},
   "source": [
    "We will again create an environment with the *Pendulum* object, like we did in the [first](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/1_environment_creation.ipynb) and [second](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/2_reset_and_step.ipynb) tutorials.\n",
    "\n",
    "Let's make the *Pendulum* object and add it to an empty graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1fbe40-73a6-4135-b7dd-6cd42b52dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum = eagerx.Object.make(\"Pendulum\", \"pendulum\", actuators=[\"u\"], sensors=[\"theta\", \"dtheta\", \"image\"], states=[\"model_state\"])\n",
    "\n",
    "# Define rate in Hz\n",
    "rate = 30.0\n",
    "\n",
    "# Initialize empty graph\n",
    "graph = eagerx.Graph.create()\n",
    "\n",
    "# Add pendulum to the graph\n",
    "graph.add(pendulum)\n",
    "\n",
    "# Connect the pendulum to an action and observation\n",
    "graph.connect(action=\"voltage\", target=pendulum.actuators.u)\n",
    "graph.connect(source=pendulum.sensors.theta, observation=\"angle\")\n",
    "graph.connect(source=pendulum.sensors.dtheta, observation=\"angular_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8f9fd-5698-4a26-8fbc-a453881444ac",
   "metadata": {},
   "source": [
    "Below we have defined a overlay node. The callback receives two inputs, namely the applied action `u` and a `raw_image`. In the callback, the applied action is visualised as an overlay on top of the raw image. The resulting `image` is the output of the callback. If we render this output, instead of the raw `pendulum.sensors.image`, we get a more informative rendered image.\n",
    "\n",
    "In the exercise of this tutorial, we will finish the code that defines the Overlay node below. Specifically, we will overlay some text on top of the image that indicates the current timestamp.\n",
    "\n",
    "Similar to what we covered in [this](https://colab.research.google.com/github/eager-dev/eagerx_tutorials/blob/master/tutorials/pendulum/4_nodes.ipynb) tutorial, we can create this node by inheriting from the class [`Node`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html).\n",
    "This class has the following abstract methods we need to implement:\n",
    "\n",
    "- [`spec()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.spec): Specifies the parameters of the node.\n",
    "- [`initialize()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.initialize): Initializes the node.\n",
    "- [`reset()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.reset): Resets the node at the beginning of an episode.\n",
    "- [`callback()`](https://eagerx.readthedocs.io/en/master/guide/api_reference/node/node.html#eagerx.core.entities.Node.callback): Called at the rate of the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878e4395-cb31-40ed-9974-b4b82ae0c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eagerx.utils.utils import Msg\n",
    "from std_msgs.msg import Float32MultiArray\n",
    "from sensor_msgs.msg import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def convert_to_cv_image(img):\n",
    "    if isinstance(img.data, bytes):\n",
    "        cv_image = np.frombuffer(img.data, dtype=np.uint8).reshape(img.height, img.width, -1)\n",
    "    else:\n",
    "        cv_image = np.array(img.data, dtype=np.uint8).reshape(img.height, img.width, -1)\n",
    "    if \"rgb\" in img.encoding:\n",
    "        cv_image = cv2.cvtColor(cv_image, cv2.COLOR_RGB2BGR)\n",
    "    return cv_image\n",
    "\n",
    "\n",
    "class Overlay(eagerx.Node):\n",
    "    @staticmethod\n",
    "    @eagerx.register.spec(\"Overlay\", eagerx.Node)\n",
    "    def spec(\n",
    "        spec: eagerx.specs.NodeSpec,\n",
    "        name: str,\n",
    "        rate: float,\n",
    "        process: int = eagerx.process.ENVIRONMENT,\n",
    "        color: str = \"cyan\",\n",
    "    ):\n",
    "        \"\"\"Overlay spec\"\"\"\n",
    "        # Fills spec with defaults parameters\n",
    "        spec.initialize(Overlay)\n",
    "\n",
    "        # Adjust default params\n",
    "        spec.config.update(name=name, rate=rate, process=process, color=color, inputs=[\"raw_image\", \"u\"], outputs=[\"image\"])\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Nothing to initialize\"\"\"\n",
    "        pass\n",
    "\n",
    "    @eagerx.register.states()\n",
    "    def reset(self):\n",
    "        \"\"\"Nothing to reset (i.e. stateless node)\"\"\"\n",
    "        pass\n",
    "\n",
    "    @eagerx.register.inputs(raw_image=Image, u=Float32MultiArray)\n",
    "    @eagerx.register.outputs(image=Image)\n",
    "    def callback(self, t_n: float, raw_image: Msg, u: Msg):\n",
    "        \"\"\"The callback where we overlay the additional info\"\"\"        \n",
    "        # Convert action message to a float\n",
    "        u = u.msgs[-1].data[0]\n",
    "\n",
    "        # Set background image from raw_image\n",
    "        img = convert_to_cv_image(raw_image.msgs[-1])\n",
    "        \n",
    "        # Get dimensions of the image\n",
    "        width = raw_image.msgs[-1].width\n",
    "        height = raw_image.msgs[-1].height\n",
    "        height_bar = int(0.05 * height)\n",
    "        width_bar = width - 2 * height_bar\n",
    "\n",
    "        # Put text\n",
    "        img = cv2.putText(img, \"Applied Voltage\", (179, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "        # Draw grey bar\n",
    "        top_left = int(0.05 * height), int(0.05 * height)\n",
    "        lower_right = int(width - height_bar), int(2 * 0.05 * height)\n",
    "        img = cv2.rectangle(img, top_left, lower_right, (125, 125, 125), -1)\n",
    "\n",
    "        # Fill bar proportional to the action that is applied\n",
    "        top_left = int(width / 2), int(0.05 * height)\n",
    "        lower_right = int(width / 2 + width_bar // 2 * u / 3),  int(2 * 0.05 * height)\n",
    "        img = cv2.rectangle(img, top_left, lower_right, (0, 0, 0), -1)\n",
    "        \n",
    "        # Add text with timestamp to image (i.e. visualise t_n)\n",
    "        # you can position it at (10, height - 20) for instance\n",
    "        # (bottom left)\n",
    "        # START EXERCISE 1.3\n",
    "        \n",
    "        # START EXERCISE 1.3\n",
    "        \n",
    "        # Prepare image for transmission.\n",
    "        data = img.tobytes(\"C\")\n",
    "        msg = Image(data=data, height=height, width=width, encoding=\"bgr8\", step=3 * width)\n",
    "        return dict(image=msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e352c4-b765-4f3b-b870-358abe3b3916",
   "metadata": {},
   "source": [
    "Below, we wil initially render the raw images produced by the pendulum image sensor as we did in the preceding tutorials. In the exercise of this tutorial, we will:\n",
    "- Add the overlay node to the graph.\n",
    "- Connect the inputs of the overlay node. I.e. `u` to the `voltage` action and `pendulum.sensors.image` to the `overlay.inputs.raw_image`.\n",
    "- Change the render source from `pendulum.sensors.image` to `overlay.outputs.raw_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858c3f13-6c63-4f92-88d3-872220769fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the overlay node\n",
    "overlay = eagerx.Node.make(\"Overlay\", \"overlay\", rate)\n",
    "\n",
    "# Copy the space converter of the actuator to the overlay node.\n",
    "# This only suppresses a warning, that is otherwise harmless.\n",
    "overlay.inputs.u.space_converter = pendulum.actuators.u.space_converter\n",
    "\n",
    "# Add overlay node to graph and connect it\n",
    "# START EXERCISE 1.1\n",
    "# Add the overlay node to the graph and connect the inputs overlay.inputs.raw_image \n",
    "# and overlay.inputs.u to pendulum.sensors.image and action voltage, respectively.\n",
    "\n",
    "# END EXERCISE 1.1\n",
    "\n",
    "# Define the render source\n",
    "# START EXERCISE 1.2\n",
    "# Change the render source to overlay.outputs.image.\n",
    "graph.render(source=pendulum.sensors.image, rate=rate)\n",
    "# START EXERCISE 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28cb4b-a564-43c9-98bb-c23259e5369f",
   "metadata": {},
   "source": [
    "We will intially use the OdeEngine, as we did in the preceding tutorials. Later on in the exercises. we will switch and use the [GymEngine](https://github.com/eager-dev/eagerx/blob/master/eagerx/engines/openai_gym/engine.py) instead.\n",
    "\n",
    "Because the overlay node is agnostic to the *engine-specific* implementation of the pendulum object, it will naturally overlay the additional information on whatever image it receives. Hence, this allows informative rendering to be available, whatever physics-engine is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18b5438",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Register both engines\n",
    "import eagerx_ode  # Registers OdeEngine\n",
    "import eagerx.engines.openai_gym  # Registers GymEngine\n",
    "\n",
    "# Make the engine\n",
    "# START EXERCISE 1.4\n",
    "engine = eagerx.Engine.make(\"OdeEngine\", rate=rate)\n",
    "# engine = eagerx.Engine.make(\"GymEngine\", rate=rate, process=eagerx.process.ENVIRONMENT)\n",
    "# graph.remove_component(pendulum.states.model_state)\n",
    "# END EXERCISE 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a4dd8-9a01-47fc-bbf8-b24a6e716a8e",
   "metadata": {},
   "source": [
    "At this point, we have create a graph containing the pendulum. We provide the graph to the environment together with the engine. Based on this engine, we will initialize the *engine-specific implementation* for the pendulum that was registered with this engine. \n",
    "- If we use the [OdeEngine](https://github.com/eager-dev/eagerx_ode), the raw sensor images are produced by the custom render function in the registered OdeEngine implementation [here](https://github.com/eager-dev/eagerx_tutorials/blob/3ddc2eb7558c7825095611fec3a01a47f5e7af79/eagerx_tutorials/pendulum/objects.py#L108-L168).\n",
    "- If we use the [GymEngine](https://github.com/eager-dev/eagerx/blob/master/eagerx/engines/openai_gym/engine.py), the raw sensor images are produced by the [Pendulum-v1](https://gym.openai.com/envs/Pendulum-v0/) environment.\n",
    "- If we would have an implemention for the real-world and registered it with the [RealEngine](https://github.com/eager-dev/eagerx_reality/blob/m1aster/eagerx_reality/engine.py), the raw sensor images could, for example, be produced by a real camera.\n",
    "\n",
    "Finally, we train the agent using [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/), again similar to the preceding tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9d2bf3-8a66-4605-a31f-e785e0a17b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN] [1652877821.045856]: eagerx.EagerxEnv will be removed in the next release. Please subclass eagerx.BaseEnv instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1652877821.108543]: Node \"/PendulumEnv/env/supervisor\" initialized.\n",
      "[INFO] [1652877821.253093]: Node \"/PendulumEnv/engine\" initialized.\n",
      "[INFO] [1652877821.377069]: Node \"/PendulumEnv/environment\" initialized.\n",
      "[INFO] [1652877821.471617]: Node \"/PendulumEnv/pendulum/theta\" initialized.\n",
      "[INFO] [1652877821.517694]: Waiting for nodes \"['env/render']\" to be initialized.\n",
      "[INFO] [1652877821.525162]: Node \"/PendulumEnv/pendulum/dtheta\" initialized.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "[INFO] [1652877821.593940]: Adding object \"pendulum\" of type \"Pendulum\" to the simulator.\n",
      "[INFO] [1652877821.614395]: Node \"/PendulumEnv/pendulum/x\" initialized.\n",
      "[INFO] [1652877821.635387]: Node \"/PendulumEnv/pendulum/image\" initialized.\n",
      "[INFO] [1652877821.635939]: [pendulum/image] START RENDERING!\n",
      "[INFO] [1652877821.653010]: Node \"/PendulumEnv/pendulum/u\" initialized.\n",
      "[INFO] [1652877821.666089]: Node \"/PendulumEnv/pendulum/u_applied\" initialized.\n",
      "[INFO] [1652877822.746353]: Nodes initialized.\n",
      "[INFO] [1652877822.803332]: Pipelines initialized.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.47e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 59        |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 404       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 29.6      |\n",
      "|    critic_loss     | 42.9      |\n",
      "|    ent_coef        | 0.916     |\n",
      "|    ent_coef_loss   | -0.117    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 303       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 808       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.2      |\n",
      "|    critic_loss     | 43.8      |\n",
      "|    ent_coef        | 0.83      |\n",
      "|    ent_coef_loss   | -0.197    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 707       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.22e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 19        |\n",
      "|    total_timesteps | 1212      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 61.1      |\n",
      "|    critic_loss     | 32.4      |\n",
      "|    ent_coef        | 0.752     |\n",
      "|    ent_coef_loss   | -0.3      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1111      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | -1.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 26        |\n",
      "|    total_timesteps | 1616      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 78.2      |\n",
      "|    critic_loss     | 33.4      |\n",
      "|    ent_coef        | 0.674     |\n",
      "|    ent_coef_loss   | -0.404    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1515      |\n",
      "----------------------------------\n",
      "[INFO] [1652877851.055585]: [PendulumEnv] Send termination signal to '/PendulumEnv/env/render'.\n",
      "[INFO] [1652877851.056500]: [PendulumEnv][/PendulumEnv/engine] Shutting down.\n",
      "[INFO] [1652877851.057221]: [/PendulumEnv/engine] Shutting down '/PendulumEnv/pendulum/x'.\n",
      "[INFO] [1652877851.057998]: [/PendulumEnv/pendulum/x] Shutting down.\n",
      "[INFO] [1652877851.108459]: [/PendulumEnv/engine] Shutting down '/PendulumEnv/pendulum/image'.\n",
      "[INFO] [1652877851.109143]: [/PendulumEnv/pendulum/image] Shutting down.\n",
      "[INFO] [1652877851.110259]: [/PendulumEnv/engine] Shutting down '/PendulumEnv/pendulum/u'.\n",
      "[INFO] [1652877851.110899]: [/PendulumEnv/pendulum/u] Shutting down.\n",
      "[INFO] [1652877851.111579]: [/PendulumEnv/engine] Shutting down '/PendulumEnv/pendulum/u_applied'.\n",
      "[INFO] [1652877851.112200]: [/PendulumEnv/pendulum/u_applied] Shutting down.\n",
      "[INFO] [1652877851.112817]: [/PendulumEnv/engine] Shutting down.\n",
      "[INFO] [1652877851.114083]: [PendulumEnv][/PendulumEnv/pendulum/theta] Shutting down.\n",
      "[INFO] [1652877851.114651]: [/PendulumEnv/pendulum/theta] Shutting down.\n",
      "[INFO] [1652877851.115244]: [PendulumEnv][/PendulumEnv/pendulum/dtheta] Shutting down.\n",
      "[INFO] [1652877851.115844]: [/PendulumEnv/pendulum/dtheta] Shutting down.\n",
      "[INFO] [1652877851.116591]: [/PendulumEnv/env/supervisor] Shutting down.\n",
      "[INFO] [1652877851.119767]: [/PendulumEnv/environment] Shutting down.\n",
      "[INFO] [1652877851.121744]: Parameters under namespace \"/PendulumEnv\" deleted.\n",
      "[INFO] [1652877822.463069]: START RENDERING!\n",
      "[INFO] [1652877822.477400]: Node \"/PendulumEnv/env/render\" initialized.\n",
      "[INFO] [1652877851.056697]: [/PendulumEnv/env/render] Shutting down.\n",
      "shutdown request: [/eagerx_core] Reason: new node registered with same name\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import stable_baselines3 as sb3\n",
    "from eagerx.wrappers import Flatten\n",
    "\n",
    "# Define step function\n",
    "def step_fn(prev_obs: Dict[str, np.ndarray], obs: Dict[str, np.ndarray], action: Dict[str, np.ndarray], steps: int):\n",
    "    \n",
    "    # Get angle and angular velocity\n",
    "    # Take first element because of window size (covered in other tutorial)\n",
    "    th = obs[\"angle\"][0] \n",
    "        \n",
    "    thdot = obs[\"angular_velocity\"][0]\n",
    "    \n",
    "    # Convert from numpy array to float\n",
    "    u = float(action[\"voltage\"])\n",
    "    \n",
    "    # Calculate cost\n",
    "    # Penalize angle error, angular velocity and input voltage\n",
    "    cost = th**2 + 0.1 * thdot**2 + 0.001 * u**2  \n",
    "    \n",
    "    # Determine when is the episode over\n",
    "    # currently just a timeout after 100 steps\n",
    "    done = steps > 100\n",
    "    \n",
    "    # Set info, tell the algorithm the termination was due to a timeout\n",
    "    # (the episode was truncated)\n",
    "    info = {\"TimeLimit.truncated\": steps > 100}\n",
    "    \n",
    "    return obs, -cost, done, info\n",
    "\n",
    "# Initialize Environment\n",
    "env = eagerx.EagerxEnv(name=\"PendulumEnv\", rate=rate, graph=graph, engine=engine, step_fn=step_fn)\n",
    "\n",
    "# Toggle render\n",
    "env.render(\"human\")\n",
    "\n",
    "# Stable Baselines3 expects flattened actions & observations\n",
    "# Convert observation and action space from Dict() to Box()\n",
    "env = Flatten(env)\n",
    "\n",
    "# Initialize learner\n",
    "model = sb3.SAC(\"MlpPolicy\", env, verbose=1, device=\"cpu\")\n",
    "\n",
    "# Train for 1 minute (sim time)\n",
    "model.learn(total_timesteps=int(60 * rate))\n",
    "\n",
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872df1d1-bed3-4b09-80dd-44c8b71a502d",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise you will create a node that overlays the applied actions over raw images that are produced by the image sensor of the pendulum. As the overlay node is agnostic to the physics-engine, we have the same overlay in every physics-engine.\n",
    "\n",
    "For this exercise, you will need to modify or add some lines of code in the cells above.\n",
    "These lines are indicated by the following comments:\n",
    "\n",
    "```python\n",
    "# START EXERCISE [BLOCK_NUMBER]\n",
    "\n",
    "# END EXERCISE [BLOCK_NUMBER]\n",
    "```\n",
    "\n",
    "However, feel free to play with the other code as well if you are interested.\n",
    "We recommend you to restart and run all code after each section (in Colab there is the option *Restart and run all* under *Runtime*).\n",
    "\n",
    "## 1. Render more informative images\n",
    "\n",
    "\n",
    "### Add your code to the following blocks: \n",
    "\n",
    "1.1 Add the overlay node to the graph and connect the inputs `overlay.inputs.raw_image` and `overlay.inputs.u` to `pendulum.sensors.image` and action `voltage`, respectively.  \n",
    "1.2 Change the render source to `overlay.outputs.image`. Using the [*eagerx_gui* package](https://github.com/eager-dev/eagerx_gui), you would see that the graph looks as below if `graph.gui()` would be called. Run the code, and you should now see the rendered overlay instead of the raw sensor images. \n",
    "\n",
    "<img src=\"./figures/tutorial_6_gui.svg\" width=720>\n",
    "\n",
    "1.3 In the callback of the overlay node, add the current time (i.e. `t_n`) as text to the image. Run the code, and you should see a timestamp that increase while the episode progresses.  \n",
    "1.4 Select the GymEngine by uncommenting the marked line. Also deselect the `model_state` by uncommenting the marked line of code.  Run the code, and you should see that the raw image has changed, but the overlay is still put on top. Hence, this demonstrates the agnostic behavior of the `graph`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
